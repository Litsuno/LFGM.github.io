{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb5yiH5h8x3h"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMGdicu8PVD9"
      },
      "source": [
        "# 使用Gemini进行笔记整理\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 准备工作"
      ],
      "metadata": {
        "id": "-AXWO9R85Jae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IbKkL5ksQYq1",
        "outputId": "83e91709-28bc-4f92-c49c-8c0ba754893c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "%pip install -U -q \"google-genai>=1.16.0\"\n",
        "%pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0H_lRdlrQYq3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "MODEL_ID=\"gemini-2.5-pro\" # @param [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash\", \"gemini-2.5-pro\", \"gemini-2.5-flash-preview-05-20\", \"gemini-2.5-pro-preview-06-05\"] {\"allow-input\":true, isTemplate: true}\n",
        "import json\n",
        "from PIL import Image\n",
        "from IPython.display import display, Markdown\n",
        "# 文件读取\n",
        "from docx import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  1. 从音频提取知识点，得到详细版笔记"
      ],
      "metadata": {
        "id": "V0mUjsKE_C2t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4b1e4d-3bbc-4625-bfe2-f2cd2d902438",
        "id": "3jT5kayXKnrb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "保存完成：result.txt\n"
          ]
        }
      ],
      "source": [
        "# GRE版\n",
        "\n",
        "# 加载 Word 文档\n",
        "doc = Document('/content/GRE单词地基词全集精讲 ｜ 北大学姐真人带背&串讲考点 ｜ 比GRE3000&佛脚词汇更高效 p03 S2-38个单词 - P03_原文.docx')\n",
        "\n",
        "# 打开 txt 文件准备写入\n",
        "with open('output.txt', 'w', encoding='utf-8') as txt_file:\n",
        "    # 遍历文档中的所有段落\n",
        "    for para in doc.paragraphs:\n",
        "        txt_file.write(para.text + '\\n')  # 将每个段落写入 txt 文件并换行\n",
        "\n",
        "\n",
        "filename = r\"output.txt\"\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    text_content = f.read()\n",
        "\n",
        "# 构造 prompt\n",
        "prompt = f\"\"\"\n",
        "请将以下课程音频的转写文本进行整理与提炼，提取出每个单词的知识点，输出为结构化的 Markdown 文件。\n",
        "\n",
        "格式要求如下：\n",
        "\n",
        "单词（Word）\n",
        "【释】[词性]. [含义1]；[词性]. [含义2]（如有多个含义，均列出；若为动词，请标明 vt. / vi.）\n",
        "【例】\n",
        "“[例句1]” ——（解释例句中该单词的含义）\n",
        "“[例句2]” ——（展示固定搭配或常用短语）\n",
        "【记】说明记忆方式（如词根词缀、联想、场景记忆等）\n",
        "【衍】列出主要衍生词，并说明词性和含义\n",
        "【同】列出常见同义词（如有细微语义区别，可补充简要辨析）\n",
        "【反】列出常见反义词\n",
        "\n",
        "每个单词之间请用三条连字符 --- 分隔，并在前后各保留一个空行。\n",
        "\n",
        "处理规则：\n",
        "\n",
        "仅根据音频内容提取知识点，不添加未提及信息；\n",
        "\n",
        "若某项内容未出现，请直接省略该行；\n",
        "\n",
        "所有输出必须格式对齐、整洁、无多余缩进；\n",
        "\n",
        "输出为纯 Markdown 文本，便于导入 Notion / Obsidian。\n",
        "{text_content}\n",
        "\"\"\"\n",
        "thinking_budget = 4096 # @param {type:\"slider\", min:0, max:24576, step:1}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=thinking_budget\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# 将 response.text 保存为 result.txt 文件\n",
        "with open(\"result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "print(\"保存完成：result.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3CAp9YrQYq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4b1e4d-3bbc-4625-bfe2-f2cd2d902438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "保存完成：result.txt\n"
          ]
        }
      ],
      "source": [
        "# TOFEL版\n",
        "\n",
        "# 加载 Word 文档\n",
        "doc = Document('/content/GRE单词地基词全集精讲 ｜ 北大学姐真人带背&串讲考点 ｜ 比GRE3000&佛脚词汇更高效 p01 S1-57个单词 - P01_原文.docx')\n",
        "\n",
        "# 打开 txt 文件准备写入\n",
        "with open('output.txt', 'w', encoding='utf-8') as txt_file:\n",
        "    # 遍历文档中的所有段落\n",
        "    for para in doc.paragraphs:\n",
        "        txt_file.write(para.text + '\\n')  # 将每个段落写入 txt 文件并换行\n",
        "\n",
        "\n",
        "filename = r\"output.txt\"\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    text_content = f.read()\n",
        "\n",
        "# 构造 prompt\n",
        "prompt = f\"\"\"\n",
        "请将以下的课程音频转写文件进行处理，提取知识点，整理成结构化的Markdown文件。格式要求如下：\n",
        "------\n",
        "\n",
        "# 单词（Word）\n",
        "\n",
        "- **meanings**：\n",
        "  - [词性]. [含义1]\n",
        "  - [词性]. [含义2]（如有多个意思，均列出）\n",
        "- **usage**：\n",
        "  - \"[例句1]\"（本单词在该例句中的含义）\n",
        "  - \"[例句2]\"（固定搭配或用法示例）\n",
        "     （每条分行，条理清晰）\n",
        "- **related words**：\n",
        "  - **SYN**（近义词）：\n",
        "    - [近义词1]（词性. 含义）\n",
        "      - [例句或补充说明]（如果用法超过两条，另起二级标题详细说明）\n",
        "    - [近义词2]（词性. 含义）\n",
        "      - [例句或补充说明]\n",
        "  - **ANTONYM**（反义词）：\n",
        "    - [反义词1]（词性. 含义）\n",
        "  - **DERIVATIVE WORDS**（衍生词）：\n",
        "    - [衍生词1]（词性. 含义）\n",
        "    - [衍生词2]（词性. 含义）\n",
        "      - [例句或补充说明]\n",
        "- **mnemonic**：\n",
        "  - 词根`[词根]` = [词根含义]，解释词义来源或联想\n",
        "  - 其他记忆方法或助记技巧\n",
        "- **notes**：\n",
        "  - [补充说明、用法注意或其他无法归类内容]\n",
        "\n",
        "------\n",
        "\n",
        "**补充说明：**\n",
        "\n",
        "- 若某一属性（meanings、usage、related words、mnemonic、notes）没有内容，则该属性可省略，不必空列。\n",
        "\n",
        "- 若related word中某词的usage超过两条，需在该词下另用二级标题写出，且保持相同结构（meanings、usage、related words、mnemonic、notes）。\n",
        "- 所有用例、短语等均需分行列出，确保阅读方便。\n",
        "- 若某知识点不能归入 meanings、usage、related words、mnemonic 四类，则归入 notes。\n",
        "\n",
        "{text_content}\n",
        "\"\"\"\n",
        "thinking_budget = 4096 # @param {type:\"slider\", min:0, max:24576, step:1}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=thinking_budget\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# 将 response.text 保存为 result.txt 文件\n",
        "with open(\"result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "print(\"保存完成：result.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.详细版笔记精炼化\n",
        "### 2.1 传入当天单词总表，分批次输入"
      ],
      "metadata": {
        "id": "PPCeIb5w5hGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import shutil\n",
        "def split_md_by_sr(filename, output_dir,split_pattern=r'(SR\\d+)'):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # 匹配 SR后面跟数字，捕获分割点\n",
        "    pattern = re.compile(split_pattern)\n",
        "    parts = pattern.split(content)\n",
        "    for i, part in enumerate(parts):\n",
        "      print(f\"parts[{i}]: {repr(part)}\")\n",
        "\n",
        "    # parts格式： ['', 'SR1', 内容1, 'SR2', 内容2, ...]\n",
        "    for i in range(1, len(parts), 2):\n",
        "        title = parts[i].strip()  # SR编号\n",
        "        body = parts[i+1].strip() if (i+1) < len(parts) else ''\n",
        "\n",
        "        # 生成文件名，比如 SR1.txt\n",
        "        out_path = os.path.join(output_dir, f\"{title}.txt\")\n",
        "\n",
        "        with open(out_path, 'w', encoding='utf-8') as fout:\n",
        "            # 保留编号作为第一行\n",
        "            fout.write(f\"{title}\\n{body}\")\n",
        "\n",
        "    print(f\"拆分完成，文件保存在：{output_dir}\")\n",
        "\n",
        "input_file = r\"/content/SR11.txt\"\n",
        "output_folder = r\"/content/files\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "    print(f\"文件夹已创建：{output_folder}\")\n",
        "else:\n",
        "    print(f\"文件夹已存在：{output_folder}\")\n",
        "\n",
        "# 删除文件夹里所有内容（文件和子文件夹）\n",
        "for filename in os.listdir(output_folder):\n",
        "    file_path = os.path.join(output_folder, filename)\n",
        "    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "        os.unlink(file_path)  # 删除文件或符号链接\n",
        "    elif os.path.isdir(file_path):\n",
        "        shutil.rmtree(file_path)  # 递归删除文件夹及里面所有内容\n",
        "split_md_by_sr(input_file, output_folder, split_pattern=r'（SR\\d+）')"
      ],
      "metadata": {
        "id": "5ItulFKa52dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 汇总得到简化版笔记"
      ],
      "metadata": {
        "id": "V-hETH_g5viR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "input_dir = \"/content/files\"\n",
        "output_file = \"/content/result.txt\"\n",
        "\n",
        "# 先清空result.md（如果已存在）\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"\")\n",
        "# 重新排序\n",
        "def extract_number(filename):\n",
        "    # 用正则匹配文件名中的数字部分\n",
        "    match = re.search(r'(\\d+)', filename)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    else:\n",
        "        # 如果没有数字，返回一个大数，保证排序靠后\n",
        "        return float('inf')\n",
        "files = [f for f in os.listdir(input_dir) if f.endswith('.txt')]\n",
        "files_sorted = sorted(files, key=extract_number)\n",
        "\n",
        "thinking_budget = 4096 # @param {type:\"slider\", min:0, max:24576, step:1}\n",
        "for filename in files_sorted:\n",
        "    if not filename.endswith(\".txt\"):\n",
        "        continue\n",
        "    input_path = os.path.join(input_dir, filename)\n",
        "\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        text_content = f.read()\n",
        "    prompt = f\"\"\"\n",
        "    请你提取以下内容，并用 Markdown 格式输出：\n",
        "\n",
        "    核心单词（用三级标题 ### 单词 标明）\n",
        "    本单词的固定搭配\n",
        "\n",
        "    本单词的用法\n",
        "\n",
        "    本单词的例句\n",
        "\n",
        "    衍生词\n",
        "\n",
        "    衍生词的用法和例句\n",
        "\n",
        "    要求：\n",
        "\n",
        "    核心单词用三级标题标明，如 ### abide，每个单词只出现一次，位于该单词所有表达之前；\n",
        "\n",
        "    其他表达以有序列表形式呈现，格式如下：\n",
        "\n",
        "    markdown\n",
        "    复制\n",
        "    编辑\n",
        "    1. \"英文表达\" (知识点单词 本句中涉及的意思或用法 中文翻译)\n",
        "        * 例句（**加粗中文翻译，方便回想**）\n",
        "    只提取英文表达，\n",
        "\n",
        "    “知识点单词”、“本句中涉及的意思或用法”、“中文翻译”由你基于原文内容准确理解并用简洁中文表达，避免逐字照搬；\n",
        "\n",
        "    例句紧跟在对应表达下面，用缩进的 * 标记单独一行展示，保持例句的英文原文和标点不变；\n",
        "\n",
        "    删除所有笔记中的注释、解释、翻译等无关内容，只保留上述表达和例句；\n",
        "\n",
        "    同一表达有多条例句时，每条例句均单独列出；\n",
        "\n",
        "    输出顺序严格为：核心单词标题 → 固定搭配 → 用法 → 例句 → 衍生词 → 衍生词用法及例句；\n",
        "\n",
        "    确保中文释义精准简洁，便于记忆与回想。\n",
        "\n",
        "    示例输入（示范）：\n",
        "    pgsql\n",
        "    复制\n",
        "    编辑\n",
        "    # abound\n",
        "\n",
        "    - Meaning: Exist in large numbers\n",
        "      - Used as an intransitive verb to indicate abundance.\n",
        "      - Example: \"Stories about his travels abound\" (关于他的游历的故事有很多).\n",
        "    - Phrase: abound with\n",
        "      - Means to be full of or rich in something.\n",
        "      - Example: \"The lake abounds with fish\" (这个湖有很多鱼).\n",
        "    示例输出：\n",
        "    markdown\n",
        "    复制\n",
        "    编辑\n",
        "    ### abound\n",
        "    1. \"abound\" (动词 表示数量多)\n",
        "        * Stories about his travels abound (关于他的游历的故事有很多).\n",
        "    2. \"abound with\" (固定搭配 充满)\n",
        "        * The lake abounds with fish (这个湖有很多鱼).\n",
        "\n",
        "    {text_content}\n",
        "    \"\"\"\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=prompt,\n",
        "        config=types.GenerateContentConfig(\n",
        "            thinking_config=types.ThinkingConfig(\n",
        "                thinking_budget=thinking_budget\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "  # 追加写入统一result.md文件，且用一级标题分割\n",
        "    with open(output_file, 'a', encoding='utf-8') as fout:\n",
        "        # 以文件名（去除扩展名）作为一级标题\n",
        "        section_title = os.path.splitext(filename)[0]\n",
        "        fout.write(f\"# {section_title}\\n\\n\")\n",
        "        fout.write(response.text)\n",
        "        fout.write(\"\\n\\n\")\n",
        "    print(f\"{filename} 处理完成，结果追加写入 {output_file}\")\n",
        "\n",
        "print(\"保存完成：result.md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q07XPppopqRO",
        "outputId": "baaf9d7f-2d99-434b-b620-b9dc079c7582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SR11.txt 处理完成，结果追加写入 /content/result.md\n",
            "SR12.txt 处理完成，结果追加写入 /content/result.md\n",
            "SR13.txt 处理完成，结果追加写入 /content/result.md\n",
            "SR14.txt 处理完成，结果追加写入 /content/result.md\n",
            "SR15.txt 处理完成，结果追加写入 /content/result.md\n",
            "SR17.txt 处理完成，结果追加写入 /content/result.md\n",
            "SR18.txt 处理完成，结果追加写入 /content/result.md\n",
            "SR19.txt 处理完成，结果追加写入 /content/result.md\n",
            "SR20.txt 处理完成，结果追加写入 /content/result.md\n",
            "保存完成：result.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 文件读取\n",
        "filename = r\"/content/Art艺术.txt\"\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    text_content = f.read()\n",
        "\n",
        "# 构造 prompt\n",
        "prompt = f\"\"\"\n",
        "任务目标\n",
        "将原始英文单词列表（包含可能的音标、词性、中文释义）转换成标准 Markdown 单词表，每个词条一行，列出英文单词 / 音标 / 中文释义三部分，同时保留章节标题或类别标记。\n",
        "处理规则\n",
        "1. 词条结构\n",
        "    格式：` 单词  音标  中文释义 `\n",
        "2. 单词识别\n",
        "    单词通常位于音标之前，去除多余空格与换行。\n",
        "    复合词或短语保持原格式（如 still life）。\n",
        "    OCR 错误明显时尝试修正（如 countrvside → countryside）。\n",
        "3. 音标提取\n",
        "    识别方括号 `[]` 或斜杠 `/` 中的内容作为音标。\n",
        "    去掉混入的词性标记（如 `n`, `adj`）和多余空格。\n",
        "    若音标缺失，请给出。\n",
        "4. 中文释义\n",
        "    音标之后的中文内容作为释义。\n",
        "    保留多义项（用 `；` 分隔），删除多余空格和标点。\n",
        "    OCR 明显错误修正（如 `泊画`→`油画`，`真止的`→`真正的`，`观煮视角`→`观察视角`）。\n",
        "5. 特殊处理\n",
        "    同义词（`=`）或替代表达（`/`）分成多条独立词条。\n",
        "    删除非单词的乱码、重复标记。\n",
        "    保留章节标题和类别标记，可用 单独的一行：` Art艺术   ` 形式表示（音标和释义可留空）。\n",
        "6. 输出要求\n",
        "    按原文顺序输出词条，不做字母排序。\n",
        "    章节标题和类别标记应作为独立行保留。\n",
        "-\n",
        "{text_content}\n",
        "\"\"\"\n",
        "thinking_budget = 4096 # @param {type:\"slider\", min:0, max:24576, step:1}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=thinking_budget\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# 将 response.text 保存为 result.txt 文件\n",
        "with open(\"result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "print(\"保存完成：result.txt\")"
      ],
      "metadata": {
        "id": "uc1apo7AqHNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32977c42-100e-4613-e8d7-10330b3868f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "保存完成：result.txt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Get_started_thinking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}